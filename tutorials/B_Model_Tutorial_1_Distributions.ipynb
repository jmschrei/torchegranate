{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e425a05c",
   "metadata": {},
   "source": [
    "## Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7151e",
   "metadata": {},
   "source": [
    "author: Jacob Schreiber <br>\n",
    "contact: jmschreiber91@gmail.com\n",
    "\n",
    "Everything in pomegranate revolves around usage of probability distributions. Although these objects can be used by themselves, e.g., fit to data or given parameters and used to evaluate new examples, they are intended to be used as a part of a larger compositional model like a mixture or a hidden Markov model. Because everything in pomegranate is meant to be plug-and-play, this means that any probability distribution can be dropped into any other model. \n",
    "\n",
    "A key difference between distributions in torchegranate and those in pomegranate is that those in pomegranate are usually univariate, in that one object represents one dimension, whereas in torchegranate each distribution is multivariate. If you wanted to model several dimensions in pomegranate you would have to use an `IndependentComponentsDistribution` with many distributions dropped in, but in torchegranate you would use a single distribution object. We'll get more into that later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de49566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "numpy        : 1.23.4\n",
      "scipy        : 1.9.3\n",
      "torch        : 1.12.1\n",
      "torchegranate: 0.0.1\n",
      "\n",
      "Compiler    : GCC 11.2.0\n",
      "OS          : Linux\n",
      "Release     : 4.15.0-197-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from torchegranate.distributions import *\n",
    "\n",
    "numpy.random.seed(0)\n",
    "numpy.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -m -n -p numpy,torch,torchegranate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321909d0",
   "metadata": {},
   "source": [
    "### Initialization and Fitting\n",
    "\n",
    "Let's first look at how to create a probability distribution. If you know what parameters you want to pass in, you can do that easily. These can be in the form of lists, tuples, numpy arrays, or torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13805af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = Normal([0.3, 0.7, 1.1], [1.1, 0.3, 1.8], covariance_type='diag')\n",
    "d2 = Exponential([0.8, 1.4, 4.1])\n",
    "d3 = Categorical([[0.3, 0.2, 0.5], [0.2, 0.1, 0.7]])\n",
    "\n",
    "d11 = Normal((0.3, 0.7, 1.1), (1.1, 0.3, 1.8), covariance_type='diag')\n",
    "d12 = Normal(numpy.array([0.3, 0.7, 1.1]), numpy.array([1.1, 0.3, 1.8]), covariance_type='diag')\n",
    "d13 = Normal(torch.tensor([0.3, 0.7, 1.1]), torch.tensor([1.1, 0.3, 1.8]), covariance_type='diag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70220e",
   "metadata": {},
   "source": [
    "If you don't have parameters you can learn them directly from data. Previously, this was done using the `Distribution.from_samples` method. However, because torchegranate aims to be more like sklearn, learning directly from data should just be done using `fit`. This will derive the parameters using MLE from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d644dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.4604,  0.4067, -0.2158]),\n",
       " tensor([[ 1.3928, -0.0990,  0.0477],\n",
       "         [-0.0990,  1.0676,  0.2146],\n",
       "         [ 0.0477,  0.2146,  1.0698]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.random.seed(0)\n",
    "\n",
    "X = numpy.random.randn(15, 3)\n",
    "\n",
    "d4 = Normal().fit(X)\n",
    "d4.means, d4.covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7adb829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4500, 0.2000, 0.3500],\n",
       "        [0.4500, 0.2500, 0.3000],\n",
       "        [0.2500, 0.3500, 0.4000],\n",
       "        [0.2500, 0.2000, 0.5500]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = numpy.random.randint(3, size=(20, 4))\n",
    "\n",
    "d5 = Categorical().fit(X2)\n",
    "d5.probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80ed6a",
   "metadata": {},
   "source": [
    "Similar to sklearn any hyperparameters used for training, such as regularization, will be passed into the initialization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62024af5",
   "metadata": {},
   "source": [
    "### Probability and Log Probability\n",
    "\n",
    "All distributions can calculate probabilities and log probabilities using those respective methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7da803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.2034, -5.9343, -3.1988, -4.4541, -3.2705, -3.5384, -5.8283, -3.2755,\n",
       "        -5.7044, -4.7418, -3.2053, -5.7131, -3.5851, -4.6144, -5.6844])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d4.log_probability(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28cdb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -3.6246,  -7.6793,  -4.2986,  -3.0519,  -3.2700,  -4.0210, -10.2286,\n",
       "         -3.5409, -12.3042,  -3.7980,  -3.7762,  -6.9385,  -3.9249,  -9.5210,\n",
       "         -7.6531])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.log_probability(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d25b42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0149, 0.0026, 0.0408, 0.0116, 0.0380, 0.0291, 0.0029, 0.0378, 0.0033,\n",
       "        0.0087, 0.0405, 0.0033, 0.0277, 0.0099, 0.0034])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d4.probability(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e24986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6660e-02, 4.6230e-04, 1.3587e-02, 4.7267e-02, 3.8006e-02, 1.7935e-02,\n",
       "        3.6123e-05, 2.8986e-02, 4.5327e-06, 2.2415e-02, 2.2911e-02, 9.6973e-04,\n",
       "        1.9744e-02, 7.3294e-05, 4.7457e-04])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.probability(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be6cb8",
   "metadata": {},
   "source": [
    " Similar to initialization, these can be lists, numpy arrays, or torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc0c465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.2034, -5.9343, -3.1988, -4.4541, -3.2705, -3.5384, -5.8283, -3.2755,\n",
       "        -5.7044, -4.7418, -3.2053, -5.7131, -3.5851, -4.6144, -5.6844])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d4.log_probability(torch.tensor(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b314697",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "Although the primary way to learn parameters from data is to use the `fit` method, the underlying engine for this learning is a pair of operations: `summarize` and `from_summaries`. In `summarize`, the data is condensed into additive sufficient statistics that can be summed across batches. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49587093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.1267, 2.3821, 1.7964])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Normal()\n",
    "d.summarize(X[:5])\n",
    "\n",
    "d._xw_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "543d0533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.9055,  6.0998, -3.2373])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.summarize(X[5:])\n",
    "\n",
    "d._xw_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c8be5",
   "metadata": {},
   "source": [
    "These values would be the same if we had summarized the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e035557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.9055,  6.0998, -3.2373])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = Normal()\n",
    "d2.summarize(X)\n",
    "\n",
    "d2._xw_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1fe2b3",
   "metadata": {},
   "source": [
    "From these values, usually stored as `_w_sum` and `_xw_sum`, one can perfectly recreate the values you would get if you fit to the entire data set. You can do this with the `from_summaries` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d05fe160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.4604,  0.4067, -0.2158]), tensor([ 0.4604,  0.4067, -0.2158]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.from_summaries()\n",
    "d2.from_summaries()\n",
    "\n",
    "d.means, d2.means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31caa1d",
   "metadata": {},
   "source": [
    "We will explore these ideas more in other tutorials, and specifically how this allows us to trivially implement batching schemes for out-of-core learning and for how to fit to large data sets using limited GPU memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
